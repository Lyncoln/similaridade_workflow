{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import parsl\n",
    "from parsl import python_app\n",
    "from parsl.config import Config\n",
    "from parsl.executors.threads import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<parsl.dataflow.dflow.DataFlowKernel at 0x7f980c567430>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config(executors=[ThreadPoolExecutor()])\n",
    "parsl.load(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_per_process_memory_fraction(0.3,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def extractName(text):\n",
    "  return re.findall(r'\"name\"\\s*:\\s*\"([^\"]+)\"',text)\n",
    "\n",
    "def extractTag(text):\n",
    "  matches = re.findall(r'\\\"tags\\\": \\[(.*?)\\]', text, re.DOTALL)\n",
    "  matches = re.findall(r'\\\"(.*?)\\\"', matches[0].strip(','))\n",
    "  return matches\n",
    "\n",
    "def extractDescription(text):\n",
    "  return re.findall(r'\"description\"\\s*:\\s*\"([^\"]+)\"',text)\n",
    "\n",
    "def extractLabel(text):\n",
    "    return re.findall(r'\"label\"\\s*:\\s*\"([^\"]+)\"', text)\n",
    "\n",
    "def removeWords(text, words):\n",
    "  for word in words:\n",
    "    text = text.replace(word,\"\")\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lyncoln/Git/similaridade_workflow/workflows_galaxy\n",
      "1014\n"
     ]
    }
   ],
   "source": [
    "os.chdir(os.path.join(os.pardir,os.pardir, 'workflows_galaxy'))\n",
    "nomes_arquivos = []\n",
    "diretorio = os.getcwd()\n",
    "print(diretorio)\n",
    "for item in os.listdir(diretorio):\n",
    "    caminho_completo = os.path.join(diretorio, item)\n",
    "    if os.path.isfile(caminho_completo):\n",
    "        nomes_arquivos.append(item)\n",
    "\n",
    "print(len(nomes_arquivos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "dic_workflows = {}\n",
    "@python_app\n",
    "def processar_arquivo(arquivo):\n",
    "    # Carregar o arquivo JSON\n",
    "    with open(arquivo, 'r') as file:\n",
    "        fileName = file.name\n",
    "        dados = file.read()\n",
    "        matches_name = extractName(dados)\n",
    "        matches_tag = extractTag(dados)\n",
    "        matches_description = extractDescription(dados)\n",
    "        matches_label = extractLabel(dados)\n",
    "\n",
    "        combined_results = {\n",
    "        'Tags': matches_tag if matches_tag else [],  \n",
    "        'Descricao': ' '.join(matches_name + matches_tag + matches_description + matches_label),\n",
    "        'Json': dados\n",
    "        }\n",
    "        dic_workflows[fileName] = combined_results\n",
    "\n",
    "        return combined_results\n",
    "\n",
    "\n",
    "futures = [processar_arquivo(arquivo) for arquivo in nomes_arquivos]\n",
    "resultados = [future.result() for future in futures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para converter texto em embedding\n",
    "def text_to_embedding(text, tokenizer, device, model):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    encoded_input = {key: value.to(device) for key, value in encoded_input.items()}  # Mover tensores para o dispositivo\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    # Pegar a média dos embeddings de todos os tokens para representar o texto\n",
    "    return model_output.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()  # Mover o resultado de volta para a CPU e converter para numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(dic_workflows, orient='index').reset_index()\n",
    "\n",
    "top_x_list = list(range(3,11))  \n",
    "\n",
    "execution_times = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for top_x in top_x_list:\n",
    "    # Medir o tempo de execução\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Carregar o modelo e o tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('google-bert/bert-large-uncased')\n",
    "    model = BertModel.from_pretrained('google-bert/bert-large-uncased')\n",
    "\n",
    "    # Definir o dispositivo (GPU ou CPU)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Supondo que você já tem um DataFrame df com uma coluna 'text'\n",
    "    texts = df['Descricao'].tolist()\n",
    "\n",
    "    # Converter todos os textos para embeddings\n",
    "    embeddings = [text_to_embedding(text, tokenizer, device, model) for text in texts]\n",
    "\n",
    "    # Converter a lista de embeddings em um array 2D\n",
    "    embeddings_array = np.vstack(embeddings)\n",
    "\n",
    "    # Calcular a matriz de similaridade\n",
    "    similarity_matrix = cosine_similarity(embeddings_array)\n",
    "    \n",
    "    # Identificar os índices dos textos mais similares para cada texto\n",
    "    similar_indices = similarity_matrix.argsort(axis=1)[:, :-top_x-2:-1]  # Selecionar os top_x mais similares excluindo o próprio texto\n",
    "\n",
    "    # Remover o índice do próprio texto\n",
    "    corrected_similar_indices = []\n",
    "    corrected_similar_tags = []\n",
    "    for idx, indices in enumerate(similar_indices):\n",
    "        filtered_indices = [index for index in indices if index != idx][:top_x]  # Exclui o próprio e pega os top_x mais similares\n",
    "        filtered_tags = [df.iloc[index]['Tags'] for index in filtered_indices]  # Obter as tags dos textos mais similares\n",
    "        corrected_similar_indices.append(filtered_indices)\n",
    "        corrected_similar_tags.append(filtered_tags)\n",
    "\n",
    "    # Criar coluna no DataFrame para os índices dos textos mais similares\n",
    "    df[f'top{top_x}_descricao'] = corrected_similar_indices\n",
    "\n",
    "    # Criar coluna no DataFrame para as tags dos textos mais similares\n",
    "    df[f'top{top_x}_tags_descricao'] = corrected_similar_tags\n",
    "\n",
    "    # Calcular a média das similaridades dos textos mais similares para cada texto\n",
    "    mean_similarities = []\n",
    "    for idx, indices in enumerate(corrected_similar_indices):\n",
    "        similarities = [similarity_matrix[idx, i] for i in indices]\n",
    "        mean_similarity = np.mean(similarities)\n",
    "        mean_similarities.append(mean_similarity)\n",
    "\n",
    "    df[f'mean_similarity_top{top_x}_descricao'] = mean_similarities\n",
    "\n",
    "    # Calcular o tempo de execução\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    # Adicionar o tempo de execução ao DataFrame de tempos de execução\n",
    "    execution_times.append({'top_x': top_x, 'execution_time_descricao': execution_time})\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    #####JSON####\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Carregar o modelo e o tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('google-bert/bert-large-uncased')\n",
    "    model = BertModel.from_pretrained('google-bert/bert-large-uncased')\n",
    "\n",
    "    # Definir o dispositivo (GPU ou CPU)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Supondo que você já tem um DataFrame df com uma coluna 'text'\n",
    "    texts = df['Json'].tolist()\n",
    "\n",
    "    # Converter todos os textos para embeddings\n",
    "    embeddings = [text_to_embedding(text, tokenizer, device, model) for text in texts]\n",
    "\n",
    "    # Converter a lista de embeddings em um array 2D\n",
    "    embeddings_array = np.vstack(embeddings)\n",
    "\n",
    "    # Calcular a matriz de similaridade\n",
    "    similarity_matrix = cosine_similarity(embeddings_array)\n",
    "    \n",
    "    # Identificar os índices dos textos mais similares para cada texto\n",
    "    similar_indices = similarity_matrix.argsort(axis=1)[:, :-top_x-2:-1]  # Selecionar os top_x mais similares excluindo o próprio texto\n",
    "\n",
    "    # Remover o índice do próprio texto\n",
    "    corrected_similar_indices = []\n",
    "    corrected_similar_tags = []\n",
    "    for idx, indices in enumerate(similar_indices):\n",
    "        filtered_indices = [index for index in indices if index != idx][:top_x]  # Exclui o próprio e pega os top_x mais similares\n",
    "        filtered_tags = [df.iloc[index]['Tags'] for index in filtered_indices]  # Obter as tags dos textos mais similares\n",
    "        corrected_similar_indices.append(filtered_indices)\n",
    "        corrected_similar_tags.append(filtered_tags)\n",
    "\n",
    "    # Criar coluna no DataFrame para os índices dos textos mais similares\n",
    "    df[f'top{top_x}_json'] = corrected_similar_indices\n",
    "\n",
    "    # Criar coluna no DataFrame para as tags dos textos mais similares\n",
    "    df[f'top{top_x}_tags_json'] = corrected_similar_tags\n",
    "\n",
    "    # Calcular a média das similaridades dos textos mais similares para cada texto\n",
    "    mean_similarities = []\n",
    "    for idx, indices in enumerate(corrected_similar_indices):\n",
    "        similarities = [similarity_matrix[idx, i] for i in indices]\n",
    "        mean_similarity = np.mean(similarities)\n",
    "        mean_similarities.append(mean_similarity)\n",
    "\n",
    "    df[f'mean_similarity_top{top_x}_json'] = mean_similarities\n",
    "\n",
    "    # Calcular o tempo de execução\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    # Adicionar o tempo de execução ao DataFrame de tempos de execução\n",
    "    execution_times.append({'top_x': top_x, 'execution_time_json': execution_time})\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_embedding(text, tokenizer, device, model):\n",
    "    inputs  = tokenizer(text, return_tensors='pt', max_length=512, padding=True, truncation=True)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    embedding = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for top_x in top_x_list:\n",
    "    # Medir o tempo de execução\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Carregar o modelo e o tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('google-bert/bert-large-uncased')\n",
    "    model = BertModel.from_pretrained('google-bert/bert-large-uncased')\n",
    "\n",
    "    # Definir o dispositivo (GPU ou CPU)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Supondo que você já tem um DataFrame df com uma coluna 'text'\n",
    "    texts = df['Descricao'].tolist()\n",
    "\n",
    "    # Converter todos os textos para embeddings\n",
    "    embeddings = [text_to_embedding(text, tokenizer, device, model) for text in texts]\n",
    "\n",
    "    # Converter a lista de embeddings em um array 2D\n",
    "    embeddings_array = np.vstack(embeddings)\n",
    "\n",
    "    # Calcular a matriz de similaridade\n",
    "    similarity_matrix = cosine_similarity(embeddings_array)\n",
    "    \n",
    "    # Identificar os índices dos textos mais similares para cada texto\n",
    "    similar_indices = similarity_matrix.argsort(axis=1)[:, :-top_x-2:-1]  # Selecionar os top_x mais similares excluindo o próprio texto\n",
    "\n",
    "    # Remover o índice do próprio texto\n",
    "    corrected_similar_indices = []\n",
    "    corrected_similar_tags = []\n",
    "    for idx, indices in enumerate(similar_indices):\n",
    "        filtered_indices = [index for index in indices if index != idx][:top_x]  # Exclui o próprio e pega os top_x mais similares\n",
    "        filtered_tags = [df.iloc[index]['Tags'] for index in filtered_indices]  # Obter as tags dos textos mais similares\n",
    "        corrected_similar_indices.append(filtered_indices)\n",
    "        corrected_similar_tags.append(filtered_tags)\n",
    "\n",
    "    # Criar coluna no DataFrame para os índices dos textos mais similares\n",
    "    df[f'top{top_x}_descricao'] = corrected_similar_indices\n",
    "\n",
    "    # Criar coluna no DataFrame para as tags dos textos mais similares\n",
    "    df[f'top{top_x}_tags_descricao'] = corrected_similar_tags\n",
    "\n",
    "    # Calcular a média das similaridades dos textos mais similares para cada texto\n",
    "    mean_similarities = []\n",
    "    for idx, indices in enumerate(corrected_similar_indices):\n",
    "        similarities = [similarity_matrix[idx, i] for i in indices]\n",
    "        mean_similarity = np.mean(similarities)\n",
    "        mean_similarities.append(mean_similarity)\n",
    "\n",
    "    df[f'mean_similarity_top{top_x}_descricao'] = mean_similarities\n",
    "\n",
    "    # Calcular o tempo de execução\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    # Adicionar o tempo de execução ao DataFrame de tempos de execução\n",
    "    execution_times.append({'top_x': top_x, 'execution_time_descricao': execution_time})\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    #####JSON####\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Carregar o modelo e o tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('google-bert/bert-large-uncased')\n",
    "    model = BertModel.from_pretrained('google-bert/bert-large-uncased')\n",
    "\n",
    "    # Definir o dispositivo (GPU ou CPU)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Supondo que você já tem um DataFrame df com uma coluna 'text'\n",
    "    texts = df['Json'].tolist()\n",
    "\n",
    "    # Converter todos os textos para embeddings\n",
    "    embeddings = [text_to_embedding(text, tokenizer, device, model) for text in texts]\n",
    "\n",
    "    # Converter a lista de embeddings em um array 2D\n",
    "    embeddings_array = np.vstack(embeddings)\n",
    "\n",
    "    # Calcular a matriz de similaridade\n",
    "    similarity_matrix = cosine_similarity(embeddings_array)\n",
    "    \n",
    "    # Identificar os índices dos textos mais similares para cada texto\n",
    "    similar_indices = similarity_matrix.argsort(axis=1)[:, :-top_x-2:-1]  # Selecionar os top_x mais similares excluindo o próprio texto\n",
    "\n",
    "    # Remover o índice do próprio texto\n",
    "    corrected_similar_indices = []\n",
    "    corrected_similar_tags = []\n",
    "    for idx, indices in enumerate(similar_indices):\n",
    "        filtered_indices = [index for index in indices if index != idx][:top_x]  # Exclui o próprio e pega os top_x mais similares\n",
    "        filtered_tags = [df.iloc[index]['Tags'] for index in filtered_indices]  # Obter as tags dos textos mais similares\n",
    "        corrected_similar_indices.append(filtered_indices)\n",
    "        corrected_similar_tags.append(filtered_tags)\n",
    "\n",
    "    # Criar coluna no DataFrame para os índices dos textos mais similares\n",
    "    df[f'top{top_x}_scibert_json'] = corrected_similar_indices\n",
    "\n",
    "    # Criar coluna no DataFrame para as tags dos textos mais similares\n",
    "    df[f'top{top_x}_tags_scibert_json'] = corrected_similar_tags\n",
    "\n",
    "    # Calcular a média das similaridades dos textos mais similares para cada texto\n",
    "    mean_similarities = []\n",
    "    for idx, indices in enumerate(corrected_similar_indices):\n",
    "        similarities = [similarity_matrix[idx, i] for i in indices]\n",
    "        mean_similarity = np.mean(similarities)\n",
    "        mean_similarities.append(mean_similarity)\n",
    "\n",
    "    df[f'mean_similarity_top{top_x}_scibert_json'] = mean_similarities\n",
    "\n",
    "    # Calcular o tempo de execução\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    # Adicionar o tempo de execução ao DataFrame de tempos de execução\n",
    "    execution_times.append({'top_x': top_x, 'execution_time_scibert_json': execution_time})\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_x</th>\n",
       "      <th>execution_time_descricao</th>\n",
       "      <th>execution_time_json</th>\n",
       "      <th>execution_time_scibert_json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>36.542646</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>119.429540</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>34.639748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>121.580234</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>34.532578</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.742011</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>36.461062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.137129</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>34.816321</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.292422</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8</td>\n",
       "      <td>35.376652</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.087524</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9</td>\n",
       "      <td>35.301404</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.639460</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10</td>\n",
       "      <td>35.525186</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.742075</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>34.511220</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>119.437564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>34.606814</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>119.947977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5</td>\n",
       "      <td>34.541047</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>119.846156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6</td>\n",
       "      <td>34.835380</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.072785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7</td>\n",
       "      <td>35.243772</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.301419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8</td>\n",
       "      <td>35.054786</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.711425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>9</td>\n",
       "      <td>34.908842</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.864774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>10</td>\n",
       "      <td>35.682216</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>121.536340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    top_x  execution_time_descricao  execution_time_json  \\\n",
       "0       3                 36.542646                  NaN   \n",
       "1       3                       NaN           119.429540   \n",
       "2       4                 34.639748                  NaN   \n",
       "3       4                       NaN           121.580234   \n",
       "4       5                 34.532578                  NaN   \n",
       "5       5                       NaN           120.742011   \n",
       "6       6                 36.461062                  NaN   \n",
       "7       6                       NaN           120.137129   \n",
       "8       7                 34.816321                  NaN   \n",
       "9       7                       NaN           120.292422   \n",
       "10      8                 35.376652                  NaN   \n",
       "11      8                       NaN           120.087524   \n",
       "12      9                 35.301404                  NaN   \n",
       "13      9                       NaN           120.639460   \n",
       "14     10                 35.525186                  NaN   \n",
       "15     10                       NaN           120.742075   \n",
       "16      3                 34.511220                  NaN   \n",
       "17      3                       NaN                  NaN   \n",
       "18      4                 34.606814                  NaN   \n",
       "19      4                       NaN                  NaN   \n",
       "20      5                 34.541047                  NaN   \n",
       "21      5                       NaN                  NaN   \n",
       "22      6                 34.835380                  NaN   \n",
       "23      6                       NaN                  NaN   \n",
       "24      7                 35.243772                  NaN   \n",
       "25      7                       NaN                  NaN   \n",
       "26      8                 35.054786                  NaN   \n",
       "27      8                       NaN                  NaN   \n",
       "28      9                 34.908842                  NaN   \n",
       "29      9                       NaN                  NaN   \n",
       "30     10                 35.682216                  NaN   \n",
       "31     10                       NaN                  NaN   \n",
       "\n",
       "    execution_time_scibert_json  \n",
       "0                           NaN  \n",
       "1                           NaN  \n",
       "2                           NaN  \n",
       "3                           NaN  \n",
       "4                           NaN  \n",
       "5                           NaN  \n",
       "6                           NaN  \n",
       "7                           NaN  \n",
       "8                           NaN  \n",
       "9                           NaN  \n",
       "10                          NaN  \n",
       "11                          NaN  \n",
       "12                          NaN  \n",
       "13                          NaN  \n",
       "14                          NaN  \n",
       "15                          NaN  \n",
       "16                          NaN  \n",
       "17                   119.437564  \n",
       "18                          NaN  \n",
       "19                   119.947977  \n",
       "20                          NaN  \n",
       "21                   119.846156  \n",
       "22                          NaN  \n",
       "23                   120.072785  \n",
       "24                          NaN  \n",
       "25                   120.301419  \n",
       "26                          NaN  \n",
       "27                   120.711425  \n",
       "28                          NaN  \n",
       "29                   120.864774  \n",
       "30                          NaN  \n",
       "31                   121.536340  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution_times_df = pd.DataFrame(execution_times)\n",
    "execution_times_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "os.chdir(os.pardir)\n",
    "df.to_csv(\"topx.csv\")\n",
    "execution_times_df.to_csv(\"topx_times.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_linux",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
