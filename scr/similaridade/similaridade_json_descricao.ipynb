{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import parsl\n",
    "from parsl import python_app\n",
    "from parsl.config import Config\n",
    "from parsl.executors.threads import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time \n",
    "import threading\n",
    "import psutil\n",
    "import GPUtil\n",
    "from time import sleep\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_resources(interval, path_name):\n",
    "    while not stop_thread.is_set():\n",
    "        # Obter o timestamp atual\n",
    "        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # Monitorando CPU e memória\n",
    "        cpu_usage = psutil.cpu_percent(interval=interval)\n",
    "        memory_info = psutil.virtual_memory()\n",
    "        memory_usage = memory_info.used / (1024 ** 2)  # Convertendo de bytes para MB\n",
    "\n",
    "        # Monitorando GPU\n",
    "        gpus = GPUtil.getGPUs()\n",
    "        gpu_info = [(gpu.load, gpu.memoryUsed) for gpu in gpus]\n",
    "        gpu_load = gpu_info[0][0]\n",
    "        gpu_memory_used = gpu_info[0][1]\n",
    "\n",
    "        new_row = pd.DataFrame([{\n",
    "            'Timestamp': current_time,\n",
    "            'CPU': cpu_usage,\n",
    "            'Memory': memory_usage,\n",
    "            'GPU': gpu_load,\n",
    "            'GPU_Memory': gpu_memory_used\n",
    "        }])\n",
    "\n",
    "        if not os.path.isfile(path_name):\n",
    "            df = pd.DataFrame(columns=[\"Timestamp\", \"CPU\", \"Memory\", \"GPU\", \"GPU_Memory\"])\n",
    "            df.to_csv(path_name, index=False)\n",
    "        else:\n",
    "            df = pd.read_csv(path_name)\n",
    "\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "        df.to_csv(path_name, index=False)\n",
    "\n",
    "        # Limpando a memória\n",
    "        del df\n",
    "\n",
    "        sleep(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "torch.cuda.set_per_process_memory_fraction(0.99)\n",
    "config = Config(executors=[ThreadPoolExecutor()])\n",
    "parsl.load(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def extractName(text):\n",
    "  return re.findall(r'\"name\"\\s*:\\s*\"([^\"]+)\"',text)\n",
    "\n",
    "def extractTag(text):\n",
    "  matches = re.findall(r'\\\"tags\\\": \\[(.*?)\\]', text, re.DOTALL)\n",
    "  matches = re.findall(r'\\\"(.*?)\\\"', matches[0].strip(','))\n",
    "  return matches\n",
    "\n",
    "def extractDescription(text):\n",
    "  return re.findall(r'\"description\"\\s*:\\s*\"([^\"]+)\"',text)\n",
    "\n",
    "def extractLabel(text):\n",
    "    return re.findall(r'\"label\"\\s*:\\s*\"([^\"]+)\"', text)\n",
    "\n",
    "def removeWords(text, words):\n",
    "  for word in words:\n",
    "    text = text.replace(word,\"\")\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(os.pardir,os.pardir, 'workflows_galaxy'))\n",
    "nomes_arquivos = []\n",
    "diretorio = os.getcwd()\n",
    "print(diretorio)\n",
    "for item in os.listdir(diretorio):\n",
    "    caminho_completo = os.path.join(diretorio, item)\n",
    "    if os.path.isfile(caminho_completo):\n",
    "        nomes_arquivos.append(item)\n",
    "\n",
    "print(len(nomes_arquivos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "dic_workflows = {}\n",
    "@python_app\n",
    "def processar_arquivo(arquivo):\n",
    "    # Carregar o arquivo JSON\n",
    "    with open(arquivo, 'r') as file:\n",
    "        fileName = file.name\n",
    "        dados = file.read()\n",
    "        matches_name = extractName(dados)\n",
    "        matches_tag = extractTag(dados)\n",
    "        matches_description = extractDescription(dados)\n",
    "        matches_label = extractLabel(dados)\n",
    "\n",
    "        combined_results = {\n",
    "        'Tags': matches_tag if matches_tag else [],  \n",
    "        'Descricao': ' '.join(matches_name + matches_tag + matches_description + matches_label),\n",
    "        'Json': dados\n",
    "        }\n",
    "        dic_workflows[fileName] = combined_results\n",
    "\n",
    "        return combined_results\n",
    "\n",
    "\n",
    "futures = [processar_arquivo(arquivo) for arquivo in nomes_arquivos]\n",
    "resultados = [future.result() for future in futures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para converter texto em embedding\n",
    "def text_to_embedding(text, tokenizer, device, model):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    encoded_input = {key: value.to(device) for key, value in encoded_input.items()}  # Mover tensores para o dispositivo\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    # Pegar a média dos embeddings de todos os tokens para representar o texto\n",
    "    return model_output.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()  # Mover o resultado de volta para a CPU e converter para numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")\n",
    "os.chdir(\"scr\")\n",
    "os.chdir(\"similaridade\")\n",
    "\n",
    "top_x_list = list(range(3,11))  \n",
    "rounds = list(range(1,11))\n",
    "\n",
    "execution_times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "for exec_round in rounds:\n",
    "    df = pd.DataFrame.from_dict(dic_workflows, orient='index').reset_index()\n",
    "    for top_x in top_x_list:\n",
    "        # Medir o tempo de execução\n",
    "        start_time = time.time()\n",
    "\n",
    "        stop_thread = threading.Event()\n",
    "        monitor_thread = threading.Thread(target=monitor_resources, args=(1.0,f\"hardware_info_bert/top{top_x}_descricao_round{exec_round}.csv\",))    \n",
    "        monitor_thread.daemon = True\n",
    "        monitor_thread.start()\n",
    "\n",
    "        # Carregar o modelo e o tokenizer\n",
    "        tokenizer = BertTokenizer.from_pretrained('google-bert/bert-large-uncased')\n",
    "        model = BertModel.from_pretrained('google-bert/bert-large-uncased')\n",
    "\n",
    "        # Definir o dispositivo (GPU ou CPU)\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model.to(device)\n",
    "\n",
    "        # Supondo que você já tem um DataFrame df com uma coluna 'text'\n",
    "        texts = df['Descricao'].tolist()\n",
    "\n",
    "        # Converter todos os textos para embeddings\n",
    "        embeddings = [text_to_embedding(text, tokenizer, device, model) for text in texts]\n",
    "\n",
    "        # Converter a lista de embeddings em um array 2D\n",
    "        embeddings_array = np.vstack(embeddings)\n",
    "\n",
    "        # Calcular a matriz de similaridade\n",
    "        similarity_matrix = cosine_similarity(embeddings_array)\n",
    "        \n",
    "        # Identificar os índices dos textos mais similares para cada texto\n",
    "        similar_indices = similarity_matrix.argsort(axis=1)[:, :-top_x-2:-1]  # Selecionar os top_x mais similares excluindo o próprio texto\n",
    "\n",
    "        # Remover o índice do próprio texto\n",
    "        corrected_similar_indices = []\n",
    "        corrected_similar_tags = []\n",
    "        for idx, indices in enumerate(similar_indices):\n",
    "            filtered_indices = [index for index in indices if index != idx][:top_x]  # Exclui o próprio e pega os top_x mais similares\n",
    "            filtered_tags = [df.iloc[index]['Tags'] for index in filtered_indices]  # Obter as tags dos textos mais similares\n",
    "            corrected_similar_indices.append(filtered_indices)\n",
    "            corrected_similar_tags.append(filtered_tags)\n",
    "\n",
    "        # Criar coluna no DataFrame para os índices dos textos mais similares\n",
    "        df[f'top{top_x}_descricao'] = corrected_similar_indices\n",
    "\n",
    "        # Criar coluna no DataFrame para as tags dos textos mais similares\n",
    "        df[f'top{top_x}_tags_descricao'] = corrected_similar_tags\n",
    "\n",
    "        # Calcular a média das similaridades dos textos mais similares para cada texto\n",
    "        mean_similarities = []\n",
    "        for idx, indices in enumerate(corrected_similar_indices):\n",
    "            similarities = [similarity_matrix[idx, i] for i in indices]\n",
    "            mean_similarity = np.mean(similarities)\n",
    "            mean_similarities.append(mean_similarity)\n",
    "\n",
    "        df[f'mean_similarity_top{top_x}_descricao'] = mean_similarities\n",
    "\n",
    "        # Calcular o tempo de execução\n",
    "        execution_time = time.time() - start_time\n",
    "\n",
    "        # Adicionar o tempo de execução ao DataFrame de tempos de execução\n",
    "        execution_times.append({'top_x': top_x, 'execution_time_descricao': execution_time})\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        stop_thread.set()\n",
    "        monitor_thread.join()\n",
    "\n",
    "\n",
    "        #####JSON####\n",
    "\n",
    "        start_time = time.time()\n",
    "        stop_thread = threading.Event()\n",
    "        monitor_thread = threading.Thread(target=monitor_resources, args=(1.0,f\"hardware_info_bert/top{top_x}_json_round{exec_round}.csv\",))\n",
    "        monitor_thread.daemon = True\n",
    "        monitor_thread.start()\n",
    "\n",
    "        # Carregar o modelo e o tokenizer\n",
    "        tokenizer = BertTokenizer.from_pretrained('google-bert/bert-large-uncased')\n",
    "        model = BertModel.from_pretrained('google-bert/bert-large-uncased')\n",
    "\n",
    "        # Definir o dispositivo (GPU ou CPU)\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model.to(device)\n",
    "\n",
    "        # Supondo que você já tem um DataFrame df com uma coluna 'text'\n",
    "        texts = df['Json'].tolist()\n",
    "\n",
    "        # Converter todos os textos para embeddings\n",
    "        embeddings = [text_to_embedding(text, tokenizer, device, model) for text in texts]\n",
    "\n",
    "        # Converter a lista de embeddings em um array 2D\n",
    "        embeddings_array = np.vstack(embeddings)\n",
    "\n",
    "        # Calcular a matriz de similaridade\n",
    "        similarity_matrix = cosine_similarity(embeddings_array)\n",
    "        \n",
    "        # Identificar os índices dos textos mais similares para cada texto\n",
    "        similar_indices = similarity_matrix.argsort(axis=1)[:, :-top_x-2:-1]  # Selecionar os top_x mais similares excluindo o próprio texto\n",
    "\n",
    "        # Remover o índice do próprio texto\n",
    "        corrected_similar_indices = []\n",
    "        corrected_similar_tags = []\n",
    "        for idx, indices in enumerate(similar_indices):\n",
    "            filtered_indices = [index for index in indices if index != idx][:top_x]  # Exclui o próprio e pega os top_x mais similares\n",
    "            filtered_tags = [df.iloc[index]['Tags'] for index in filtered_indices]  # Obter as tags dos textos mais similares\n",
    "            corrected_similar_indices.append(filtered_indices)\n",
    "            corrected_similar_tags.append(filtered_tags)\n",
    "\n",
    "        # Criar coluna no DataFrame para os índices dos textos mais similares\n",
    "        df[f'top{top_x}_json'] = corrected_similar_indices\n",
    "\n",
    "        # Criar coluna no DataFrame para as tags dos textos mais similares\n",
    "        df[f'top{top_x}_tags_json'] = corrected_similar_tags\n",
    "\n",
    "        # Calcular a média das similaridades dos textos mais similares para cada texto\n",
    "        mean_similarities = []\n",
    "        for idx, indices in enumerate(corrected_similar_indices):\n",
    "            similarities = [similarity_matrix[idx, i] for i in indices]\n",
    "            mean_similarity = np.mean(similarities)\n",
    "            mean_similarities.append(mean_similarity)\n",
    "\n",
    "        df[f'mean_similarity_top{top_x}_json'] = mean_similarities\n",
    "\n",
    "        # Calcular o tempo de execução\n",
    "        execution_time = time.time() - start_time\n",
    "\n",
    "        # Adicionar o tempo de execução ao DataFrame de tempos de execução\n",
    "        execution_times.append({'top_x': top_x, 'execution_time_json': execution_time})\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        stop_thread.set()\n",
    "        monitor_thread.join()\n",
    "\n",
    "    df.to_csv(f\"infos_bert/results_bert_round{exec_round}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_embedding(text, tokenizer, device, model):\n",
    "    inputs  = tokenizer(text, return_tensors='pt', max_length=512, padding=True, truncation=True)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    embedding = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exec_round in rounds:\n",
    "    df = pd.DataFrame.from_dict(dic_workflows, orient='index').reset_index()\n",
    "    for top_x in top_x_list:\n",
    "        # Medir o tempo de execução\n",
    "        start_time = time.time()\n",
    "\n",
    "        stop_thread = threading.Event()\n",
    "        monitor_thread = threading.Thread(target=monitor_resources, args=(1.0,f\"hardware_info_scibert/top{top_x}_descricao_round{exec_round}.csv\",))    \n",
    "        monitor_thread.daemon = True\n",
    "        monitor_thread.start()\n",
    "\n",
    "        # Carregar o modelo e o tokenizer\n",
    "        tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "        model = BertModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "\n",
    "        # Definir o dispositivo (GPU ou CPU)\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model.to(device)\n",
    "\n",
    "        # Supondo que você já tem um DataFrame df com uma coluna 'text'\n",
    "        texts = df['Descricao'].tolist()\n",
    "\n",
    "        # Converter todos os textos para embeddings\n",
    "        embeddings = [text_to_embedding(text, tokenizer, device, model) for text in texts]\n",
    "\n",
    "        # Converter a lista de embeddings em um array 2D\n",
    "        embeddings_array = np.vstack(embeddings)\n",
    "\n",
    "        # Calcular a matriz de similaridade\n",
    "        similarity_matrix = cosine_similarity(embeddings_array)\n",
    "        \n",
    "        # Identificar os índices dos textos mais similares para cada texto\n",
    "        similar_indices = similarity_matrix.argsort(axis=1)[:, :-top_x-2:-1]  # Selecionar os top_x mais similares excluindo o próprio texto\n",
    "\n",
    "        # Remover o índice do próprio texto\n",
    "        corrected_similar_indices = []\n",
    "        corrected_similar_tags = []\n",
    "        for idx, indices in enumerate(similar_indices):\n",
    "            filtered_indices = [index for index in indices if index != idx][:top_x]  # Exclui o próprio e pega os top_x mais similares\n",
    "            filtered_tags = [df.iloc[index]['Tags'] for index in filtered_indices]  # Obter as tags dos textos mais similares\n",
    "            corrected_similar_indices.append(filtered_indices)\n",
    "            corrected_similar_tags.append(filtered_tags)\n",
    "\n",
    "        # Criar coluna no DataFrame para os índices dos textos mais similares\n",
    "        df[f'top{top_x}_descricao'] = corrected_similar_indices\n",
    "\n",
    "        # Criar coluna no DataFrame para as tags dos textos mais similares\n",
    "        df[f'top{top_x}_tags_descricao'] = corrected_similar_tags\n",
    "\n",
    "        # Calcular a média das similaridades dos textos mais similares para cada texto\n",
    "        mean_similarities = []\n",
    "        for idx, indices in enumerate(corrected_similar_indices):\n",
    "            similarities = [similarity_matrix[idx, i] for i in indices]\n",
    "            mean_similarity = np.mean(similarities)\n",
    "            mean_similarities.append(mean_similarity)\n",
    "\n",
    "        df[f'mean_similarity_top{top_x}_descricao'] = mean_similarities\n",
    "\n",
    "        # Calcular o tempo de execução\n",
    "        execution_time = time.time() - start_time\n",
    "\n",
    "        # Adicionar o tempo de execução ao DataFrame de tempos de execução\n",
    "        execution_times.append({'top_x': top_x, 'execution_time_descricao': execution_time})\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        stop_thread.set()\n",
    "        monitor_thread.join()\n",
    "\n",
    "\n",
    "        #####JSON####\n",
    "\n",
    "        start_time = time.time()\n",
    "        stop_thread = threading.Event()\n",
    "        monitor_thread = threading.Thread(target=monitor_resources, args=(1.0,f\"hardware_info_scibert/top{top_x}_json_round{exec_round}.csv\",))\n",
    "        monitor_thread.daemon = True\n",
    "        monitor_thread.start()\n",
    "\n",
    "        # Carregar o modelo e o tokenizer\n",
    "        tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "        model = BertModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "\n",
    "        # Definir o dispositivo (GPU ou CPU)\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model.to(device)\n",
    "\n",
    "        # Supondo que você já tem um DataFrame df com uma coluna 'text'\n",
    "        texts = df['Json'].tolist()\n",
    "\n",
    "        # Converter todos os textos para embeddings\n",
    "        embeddings = [text_to_embedding(text, tokenizer, device, model) for text in texts]\n",
    "\n",
    "        # Converter a lista de embeddings em um array 2D\n",
    "        embeddings_array = np.vstack(embeddings)\n",
    "\n",
    "        # Calcular a matriz de similaridade\n",
    "        similarity_matrix = cosine_similarity(embeddings_array)\n",
    "        \n",
    "        # Identificar os índices dos textos mais similares para cada texto\n",
    "        similar_indices = similarity_matrix.argsort(axis=1)[:, :-top_x-2:-1]  # Selecionar os top_x mais similares excluindo o próprio texto\n",
    "\n",
    "        # Remover o índice do próprio texto\n",
    "        corrected_similar_indices = []\n",
    "        corrected_similar_tags = []\n",
    "        for idx, indices in enumerate(similar_indices):\n",
    "            filtered_indices = [index for index in indices if index != idx][:top_x]  # Exclui o próprio e pega os top_x mais similares\n",
    "            filtered_tags = [df.iloc[index]['Tags'] for index in filtered_indices]  # Obter as tags dos textos mais similares\n",
    "            corrected_similar_indices.append(filtered_indices)\n",
    "            corrected_similar_tags.append(filtered_tags)\n",
    "\n",
    "        # Criar coluna no DataFrame para os índices dos textos mais similares\n",
    "        df[f'top{top_x}_json'] = corrected_similar_indices\n",
    "\n",
    "        # Criar coluna no DataFrame para as tags dos textos mais similares\n",
    "        df[f'top{top_x}_tags_json'] = corrected_similar_tags\n",
    "\n",
    "        # Calcular a média das similaridades dos textos mais similares para cada texto\n",
    "        mean_similarities = []\n",
    "        for idx, indices in enumerate(corrected_similar_indices):\n",
    "            similarities = [similarity_matrix[idx, i] for i in indices]\n",
    "            mean_similarity = np.mean(similarities)\n",
    "            mean_similarities.append(mean_similarity)\n",
    "\n",
    "        df[f'mean_similarity_top{top_x}_json'] = mean_similarities\n",
    "\n",
    "        # Calcular o tempo de execução\n",
    "        execution_time = time.time() - start_time\n",
    "\n",
    "        # Adicionar o tempo de execução ao DataFrame de tempos de execução\n",
    "        execution_times.append({'top_x': top_x, 'execution_time_json': execution_time})\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        stop_thread.set()\n",
    "        monitor_thread.join()\n",
    "\n",
    "    df.to_csv(f\"infos_scibert/results_scibert_round{exec_round}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_linux",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
