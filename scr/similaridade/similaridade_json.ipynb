{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import parsl\n",
    "from parsl import python_app\n",
    "from parsl.config import Config\n",
    "from parsl.executors.threads import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<parsl.dataflow.dflow.DataFlowKernel at 0x7fd0c9d2fbe0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config(executors=[ThreadPoolExecutor()])\n",
    "parsl.load(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTag(text):\n",
    "  matches = re.findall(r'\\\"tags\\\": \\[(.*?)\\]', text, re.DOTALL)\n",
    "  matches = re.findall(r'\\\"(.*?)\\\"', matches[0].strip(','))\n",
    "  return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1014\n"
     ]
    }
   ],
   "source": [
    "#Constroi a lista com os caminhos dos workflows\n",
    "os.chdir(os.path.join(os.pardir,os.pardir, 'workflows_galaxy'))\n",
    "nomes_arquivos = []\n",
    "diretorio = os.getcwd()\n",
    "\n",
    "for item in os.listdir(diretorio):\n",
    "    caminho_completo = os.path.join(diretorio, item)\n",
    "    if os.path.isfile(caminho_completo):\n",
    "        nomes_arquivos.append(item)\n",
    "\n",
    "#nomes_arquivos = nomes_arquivos[0:10]\n",
    "\n",
    "print(len(nomes_arquivos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "dic_workflows = {}\n",
    "@python_app\n",
    "def processar_arquivo(arquivo):\n",
    "    # Carregar o arquivo JSON\n",
    "    with open(arquivo, 'r') as file:\n",
    "        fileName = file.name\n",
    "        dados = file.read()\n",
    "        matches_tag = extractTag(dados)\n",
    "        dic_workflows[fileName] = dados\n",
    "\n",
    "        combined_results = {\n",
    "        'Tags': matches_tag if matches_tag else [],  # Lidar com o cenário de não ter tags\n",
    "        'Content': dados\n",
    "        }\n",
    "\n",
    "        dic_workflows[fileName] = combined_results\n",
    "\n",
    "        return dic_workflows[fileName]\n",
    "\n",
    "\n",
    "# Inicia o processamento dos arquivos em paralelo\n",
    "futures = [processar_arquivo(arquivo) for arquivo in nomes_arquivos]\n",
    "\n",
    "resultados = [future.result() for future in futures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0c86c39dcd9e08c6.json</td>\n",
       "      <td>[]</td>\n",
       "      <td>{\\n    \"a_galaxy_workflow\": \"true\",\\n    \"anno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>692a2b0bb818336d.json</td>\n",
       "      <td>[]</td>\n",
       "      <td>{\\n    \"a_galaxy_workflow\": \"true\",\\n    \"anno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3680984663c813e1.json</td>\n",
       "      <td>[]</td>\n",
       "      <td>{\\n    \"a_galaxy_workflow\": \"true\",\\n    \"anno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49f8b32c3206f76c.json</td>\n",
       "      <td>[variant, snps, human]</td>\n",
       "      <td>{\\n    \"a_galaxy_workflow\": \"true\",\\n    \"anno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25adc55d2a26e34b.json</td>\n",
       "      <td>[]</td>\n",
       "      <td>{\\n    \"a_galaxy_workflow\": \"true\",\\n    \"anno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>d5cba8a5ba6880fd.json</td>\n",
       "      <td>[]</td>\n",
       "      <td>{\\n    \"a_galaxy_workflow\": \"true\",\\n    \"anno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>e0da87cadb1e6d5f.json</td>\n",
       "      <td>[]</td>\n",
       "      <td>{\\n    \"a_galaxy_workflow\": \"true\",\\n    \"anno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>fbf75fbb72b488bd.json</td>\n",
       "      <td>[]</td>\n",
       "      <td>{\\n    \"a_galaxy_workflow\": \"true\",\\n    \"anno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>0d8eb75d28fa7df2.json</td>\n",
       "      <td>[]</td>\n",
       "      <td>{\\n    \"a_galaxy_workflow\": \"true\",\\n    \"anno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>f8238234db6f04c3.json</td>\n",
       "      <td>[]</td>\n",
       "      <td>{\\n    \"a_galaxy_workflow\": \"true\",\\n    \"anno...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1014 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      index                    Tags  \\\n",
       "0     0c86c39dcd9e08c6.json                      []   \n",
       "1     692a2b0bb818336d.json                      []   \n",
       "2     3680984663c813e1.json                      []   \n",
       "3     49f8b32c3206f76c.json  [variant, snps, human]   \n",
       "4     25adc55d2a26e34b.json                      []   \n",
       "...                     ...                     ...   \n",
       "1009  d5cba8a5ba6880fd.json                      []   \n",
       "1010  e0da87cadb1e6d5f.json                      []   \n",
       "1011  fbf75fbb72b488bd.json                      []   \n",
       "1012  0d8eb75d28fa7df2.json                      []   \n",
       "1013  f8238234db6f04c3.json                      []   \n",
       "\n",
       "                                                Content  \n",
       "0     {\\n    \"a_galaxy_workflow\": \"true\",\\n    \"anno...  \n",
       "1     {\\n    \"a_galaxy_workflow\": \"true\",\\n    \"anno...  \n",
       "2     {\\n    \"a_galaxy_workflow\": \"true\",\\n    \"anno...  \n",
       "3     {\\n    \"a_galaxy_workflow\": \"true\",\\n    \"anno...  \n",
       "4     {\\n    \"a_galaxy_workflow\": \"true\",\\n    \"anno...  \n",
       "...                                                 ...  \n",
       "1009  {\\n    \"a_galaxy_workflow\": \"true\",\\n    \"anno...  \n",
       "1010  {\\n    \"a_galaxy_workflow\": \"true\",\\n    \"anno...  \n",
       "1011  {\\n    \"a_galaxy_workflow\": \"true\",\\n    \"anno...  \n",
       "1012  {\\n    \"a_galaxy_workflow\": \"true\",\\n    \"anno...  \n",
       "1013  {\\n    \"a_galaxy_workflow\": \"true\",\\n    \"anno...  \n",
       "\n",
       "[1014 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(dic_workflows, orient='index').reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para converter texto em embedding\n",
    "def text_to_embedding(text, tokenizer, device, model):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    encoded_input = {key: value.to(device) for key, value in encoded_input.items()}  # Mover tensores para o dispositivo\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    # Pegar a média dos embeddings de todos os tokens para representar o texto\n",
    "    return model_output.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()  # Mover o resultado de volta para a CPU e converter para numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   top_x  execution_time\n",
      "0      1      187.725738\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(dic_workflows, orient='index').reset_index()\n",
    "\n",
    "# Definir a lista de valores de tops a serem calculados\n",
    "top_x_list = list(range(1,2))  # Você pode alterar ou adicionar mais valores a esta lista\n",
    "\n",
    "# DataFrame para armazenar os tempos de execução\n",
    "execution_times = []\n",
    "\n",
    "# Calcular os top_x para cada valor na lista top_x_list\n",
    "# Calcular os top_x para cada valor na lista top_x_list\n",
    "for top_x in top_x_list:\n",
    "    # Medir o tempo de execução\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Carregar o modelo e o tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('google-bert/bert-large-uncased')\n",
    "    model = BertModel.from_pretrained('google-bert/bert-large-uncased')\n",
    "\n",
    "    # Definir o dispositivo (GPU ou CPU)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Supondo que você já tem um DataFrame df com uma coluna 'text'\n",
    "    texts = df['Content'].tolist()\n",
    "\n",
    "    # Converter todos os textos para embeddings\n",
    "    embeddings = [text_to_embedding(text, tokenizer, device, model) for text in texts]\n",
    "\n",
    "    # Converter a lista de embeddings em um array 2D\n",
    "    embeddings_array = np.vstack(embeddings)\n",
    "\n",
    "    # Calcular a matriz de similaridade\n",
    "    similarity_matrix = cosine_similarity(embeddings_array)\n",
    "    \n",
    "    # Identificar os índices dos textos mais similares para cada texto\n",
    "    similar_indices = similarity_matrix.argsort(axis=1)[:, :-top_x-2:-1]  # Selecionar os top_x mais similares excluindo o próprio texto\n",
    "\n",
    "    # Remover o índice do próprio texto\n",
    "    corrected_similar_indices = []\n",
    "    corrected_similar_tags = []\n",
    "    for idx, indices in enumerate(similar_indices):\n",
    "        filtered_indices = [index for index in indices if index != idx][:top_x]  # Exclui o próprio e pega os top_x mais similares\n",
    "        filtered_tags = [df.iloc[index]['Tags'] for index in filtered_indices]  # Obter as tags dos textos mais similares\n",
    "        corrected_similar_indices.append(filtered_indices)\n",
    "        corrected_similar_tags.append(filtered_tags)\n",
    "\n",
    "    # Criar coluna no DataFrame para os índices dos textos mais similares\n",
    "    df[f'top{top_x}_description'] = corrected_similar_indices\n",
    "\n",
    "    # Criar coluna no DataFrame para as tags dos textos mais similares\n",
    "    df[f'top{top_x}_tags'] = corrected_similar_tags\n",
    "\n",
    "    # Calcular a média das similaridades dos textos mais similares para cada texto\n",
    "    mean_similarities = []\n",
    "    for idx, indices in enumerate(corrected_similar_indices):\n",
    "        similarities = [similarity_matrix[idx, i] for i in indices]\n",
    "        mean_similarity = np.mean(similarities)\n",
    "        mean_similarities.append(mean_similarity)\n",
    "\n",
    "    df[f'mean_similarity_top{top_x}'] = mean_similarities\n",
    "\n",
    "    # Calcular o tempo de execução\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    # Adicionar o tempo de execução ao DataFrame de tempos de execução\n",
    "    execution_times.append({'top_x': top_x, 'execution_time': execution_time})\n",
    "\n",
    "# Exibir o DataFrame com os tempos de execução\n",
    "execution_times_df = pd.DataFrame(execution_times)\n",
    "print(execution_times_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Carregar o modelo e o tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "model = BertModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "\n",
    "texts = df['text'].tolist()\n",
    "\n",
    "# Função para converter texto em embedding\n",
    "def text_to_embedding(text):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt', max_length=512, padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    # Pegar a média dos embeddings de todos os tokens para representar o texto\n",
    "    return model_output.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Converter todos os textos para embeddings\n",
    "embeddings = [text_to_embedding(text) for text in texts]\n",
    "\n",
    "# Converter a lista de embeddings em um array 2D\n",
    "embeddings_array = np.vstack(embeddings)\n",
    "\n",
    "# Calcular a matriz de similaridade\n",
    "similarity_matrix = cosine_similarity(embeddings_array)\n",
    "\n",
    "# Identificar os índices dos três textos mais similares para cada texto\n",
    "similar_indices = similarity_matrix.argsort(axis=1)[:, :-4:-1]  # -4 porque os três mais similares excluem o próprio texto\n",
    "\n",
    "# Criar coluna no DataFrame para os índices dos textos mais similares\n",
    "df['top3_descrition_scibert'] = similar_indices.tolist()\n",
    "\n",
    "# Criar coluna no DataFrame para os textos mais similares\n",
    "#df['top3_similar_text_descrition_scibert'] = [[df.loc[idx, 'text'] for idx in indices] for indices in similar_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# os.chdir(os.pardir)\n",
    "df.to_csv(\"top3_json.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/lyncoln/Git'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_linux",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
