{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import parsl\n",
    "from parsl import python_app\n",
    "from parsl.config import Config\n",
    "from parsl.executors.threads import ThreadPoolExecutor\n",
    "from transformers import EarlyStoppingCallback\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<parsl.dataflow.dflow.DataFlowKernel at 0x7f1848a40790>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuração básica do Parsl com ThreadPoolExecutor\n",
    "config = Config(executors=[ThreadPoolExecutor()])\n",
    "# Start Parsl on a single computer\n",
    "parsl.load(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1014\n"
     ]
    }
   ],
   "source": [
    "#Constroi a lista com os caminhos dos workflows\n",
    "os.chdir(os.path.join(os.pardir,os.pardir, 'workflows_galaxy'))\n",
    "nomes_arquivos = []\n",
    "diretorio = os.getcwd()\n",
    "\n",
    "for item in os.listdir(diretorio):\n",
    "    caminho_completo = os.path.join(diretorio, item)\n",
    "    if os.path.isfile(caminho_completo):\n",
    "        nomes_arquivos.append(item)\n",
    "\n",
    "#nomes_arquivos = nomes_arquivos[0:10]\n",
    "\n",
    "print(len(nomes_arquivos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "dic_workflows = {}\n",
    "@python_app\n",
    "def processar_arquivo(arquivo):\n",
    "    # Carregar o arquivo JSON\n",
    "    with open(arquivo, 'r') as file:\n",
    "        fileName = file.name\n",
    "        dados = file.read()\n",
    "        dic_workflows[fileName] = dados\n",
    "\n",
    "        return dic_workflows[fileName]\n",
    "\n",
    "\n",
    "# Inicia o processamento dos arquivos em paralelo\n",
    "futures = [processar_arquivo(arquivo) for arquivo in nomes_arquivos]\n",
    "\n",
    "resultados = [future.result() for future in futures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# texto_completo = \" \".join(dic_workflows.keys()) + \" \" + \" \".join(dic_workflows.values())\n",
    "\n",
    "# # Separar o texto em palavras\n",
    "# palavras = texto_completo.split()\n",
    "\n",
    "# # Contar a frequência de cada palavra\n",
    "# frequencias = Counter(palavras)\n",
    "\n",
    "# # Criar um DataFrame a partir do dicionário de frequências\n",
    "# df = pd.DataFrame(list(frequencias.items()), columns=['Palavra', 'Frequência'])\n",
    "\n",
    "# # Ordenar o DataFrame pela frequência das palavras em ordem decrescente\n",
    "# df = df.sort_values(by='Frequência', ascending=False)\n",
    "\n",
    "# # Exibir o DataFrame\n",
    "# print(df[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       name                                               text\n",
      "0     0c86c39dcd9e08c6.json  {\\n    \"a_galaxy_workflow\": \"true\",\\n    \"anno...\n",
      "1     3680984663c813e1.json  {\\n    \"a_galaxy_workflow\": \"true\",\\n    \"anno...\n",
      "2     692a2b0bb818336d.json  {\\n    \"a_galaxy_workflow\": \"true\",\\n    \"anno...\n",
      "3     49f8b32c3206f76c.json  {\\n    \"a_galaxy_workflow\": \"true\",\\n    \"anno...\n",
      "4     25adc55d2a26e34b.json  {\\n    \"a_galaxy_workflow\": \"true\",\\n    \"anno...\n",
      "...                     ...                                                ...\n",
      "1009  d5cba8a5ba6880fd.json  {\\n    \"a_galaxy_workflow\": \"true\",\\n    \"anno...\n",
      "1010  e0da87cadb1e6d5f.json  {\\n    \"a_galaxy_workflow\": \"true\",\\n    \"anno...\n",
      "1011  fbf75fbb72b488bd.json  {\\n    \"a_galaxy_workflow\": \"true\",\\n    \"anno...\n",
      "1012  0d8eb75d28fa7df2.json  {\\n    \"a_galaxy_workflow\": \"true\",\\n    \"anno...\n",
      "1013  f8238234db6f04c3.json  {\\n    \"a_galaxy_workflow\": \"true\",\\n    \"anno...\n",
      "\n",
      "[1014 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(list(dic_workflows.items()), columns=['name', 'text'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Carregar o modelo e o tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('google-bert/bert-large-uncased')\n",
    "model = BertModel.from_pretrained('google-bert/bert-large-uncased')\n",
    "\n",
    "texts = df['text'].tolist()\n",
    "\n",
    "# Função para converter texto em embedding\n",
    "def text_to_embedding(text):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    # Pegar a média dos embeddings de todos os tokens para representar o texto\n",
    "    return model_output.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Converter todos os textos para embeddings\n",
    "embeddings = [text_to_embedding(text) for text in texts]\n",
    "\n",
    "# Converter a lista de embeddings em um array 2D\n",
    "embeddings_array = np.vstack(embeddings)\n",
    "\n",
    "# Calcular a matriz de similaridade\n",
    "similarity_matrix = cosine_similarity(embeddings_array)\n",
    "\n",
    "# Identificar o índice do texto mais similar para cada texto\n",
    "#similar_indices = similarity_matrix.argsort(axis=1)[:, -2]  # -2 porque o mais similar é o próprio texto\n",
    "\n",
    "# Identificar os índices dos três textos mais similares para cada texto\n",
    "similar_indices = similarity_matrix.argsort(axis=1)[:, :-4:-1]  # -4 porque os três mais similares excluem o próprio texto\n",
    "\n",
    "# Criar coluna no DataFrame para os índices dos textos mais similares\n",
    "df['top3_descrition'] = similar_indices.tolist()\n",
    "\n",
    "# Criar coluna no DataFrame para os textos mais similares\n",
    "#df['top3_similar_text_descrition'] = [[df.loc[idx, 'text'] for idx in indices] for indices in similar_indices]\n",
    "\n",
    "\n",
    "#df['similar_index_descrition'] = similar_indices\n",
    "#df['similar_text_descrition'] = df.loc[similar_indices, 'text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Carregar o modelo e o tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "model = BertModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "\n",
    "texts = df['text'].tolist()\n",
    "\n",
    "# Função para converter texto em embedding\n",
    "def text_to_embedding(text):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt', max_length=512, padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    # Pegar a média dos embeddings de todos os tokens para representar o texto\n",
    "    return model_output.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Converter todos os textos para embeddings\n",
    "embeddings = [text_to_embedding(text) for text in texts]\n",
    "\n",
    "# Converter a lista de embeddings em um array 2D\n",
    "embeddings_array = np.vstack(embeddings)\n",
    "\n",
    "# Calcular a matriz de similaridade\n",
    "similarity_matrix = cosine_similarity(embeddings_array)\n",
    "\n",
    "# Identificar os índices dos três textos mais similares para cada texto\n",
    "similar_indices = similarity_matrix.argsort(axis=1)[:, :-4:-1]  # -4 porque os três mais similares excluem o próprio texto\n",
    "\n",
    "# Criar coluna no DataFrame para os índices dos textos mais similares\n",
    "df['top3_descrition_scibert'] = similar_indices.tolist()\n",
    "\n",
    "# Criar coluna no DataFrame para os textos mais similares\n",
    "#df['top3_similar_text_descrition_scibert'] = [[df.loc[idx, 'text'] for idx in indices] for indices in similar_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(os.pardir,\"/scr/similaridade\")\n",
    "df.to_csv(\"top3_json.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/lyncoln/Git/similaridade_workflow'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(os.path.join(os.pardir,\"/scr/similaridade\")\n",
    "os.getcwd()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_linux",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
