{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import parsl\n",
    "from parsl import python_app\n",
    "from parsl.config import Config\n",
    "from parsl.executors.threads import ThreadPoolExecutor\n",
    "from transformers import EarlyStoppingCallback\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<parsl.dataflow.dflow.DataFlowKernel at 0x7f50f43e0eb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config(executors=[ThreadPoolExecutor()])\n",
    "parsl.load(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def extractName(text):\n",
    "  return re.findall(r'\"name\"\\s*:\\s*\"([^\"]+)\"',text)\n",
    "\n",
    "def extractTag(text):\n",
    "  matches = re.findall(r'\\\"tags\\\": \\[(.*?)\\]', text, re.DOTALL)\n",
    "  matches = re.findall(r'\\\"(.*?)\\\"', matches[0].strip(','))\n",
    "  return matches\n",
    "\n",
    "def extractDescription(text):\n",
    "  return re.findall(r'\"description\"\\s*:\\s*\"([^\"]+)\"',text)\n",
    "\n",
    "def extractLabel(text):\n",
    "    return re.findall(r'\"label\"\\s*:\\s*\"([^\"]+)\"', text)\n",
    "\n",
    "def removeWords(text, words):\n",
    "  for word in words:\n",
    "    text = text.replace(word,\"\")\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lyncoln/Git/similaridade_workflow/workflows_galaxy\n",
      "1014\n"
     ]
    }
   ],
   "source": [
    "os.chdir(os.path.join(os.pardir,os.pardir, 'workflows_galaxy'))\n",
    "nomes_arquivos = []\n",
    "diretorio = os.getcwd()\n",
    "print(diretorio)\n",
    "for item in os.listdir(diretorio):\n",
    "    caminho_completo = os.path.join(diretorio, item)\n",
    "    if os.path.isfile(caminho_completo):\n",
    "        nomes_arquivos.append(item)\n",
    "\n",
    "print(len(nomes_arquivos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "dic_workflows = {}\n",
    "@python_app\n",
    "def processar_arquivo(arquivo):\n",
    "    # Carregar o arquivo JSON\n",
    "    with open(arquivo, 'r') as file:\n",
    "        fileName = file.name\n",
    "        dados = file.read()\n",
    "        # print(dados)\n",
    "        matches_name = extractName(dados)\n",
    "        matches_tag = extractTag(dados)\n",
    "        matches_description = extractDescription(dados)\n",
    "        matches_label = extractLabel(dados)\n",
    "        combined_results = ' '.join(matches_name + matches_tag + matches_description + matches_label)\n",
    "        # combined_results = removeWords(combined_results,['input dataset(s)'])\n",
    "        dic_workflows[fileName] = combined_results\n",
    "\n",
    "        save_path = 'crawler_results'\n",
    "        os.makedirs(save_path, exist_ok=True)  # Cria a pasta se não existir\n",
    "        output_file_path = os.path.join(save_path, f\"{fileName}.txt\")\n",
    "\n",
    "        # Salvando os resultados em um arquivo\n",
    "        with open(output_file_path, 'w') as output_file:\n",
    "            output_file.write(combined_results)\n",
    "\n",
    "        return combined_results\n",
    "\n",
    "\n",
    "# Inicia o processamento dos arquivos em paralelo\n",
    "futures = [processar_arquivo(arquivo) for arquivo in nomes_arquivos]\n",
    "\n",
    "resultados = [future.result() for future in futures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# texto_completo = \" \".join(dic_workflows.keys()) + \" \" + \" \".join(dic_workflows.values())\n",
    "# palavras = texto_completo.split()\n",
    "# frequencias = Counter(palavras)\n",
    "# df = pd.DataFrame(list(frequencias.items()), columns=['Palavra', 'Frequência'])\n",
    "# df = df.sort_values(by='Frequência', ascending=False)\n",
    "# print(df[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       name                                               text\n",
      "0     0c86c39dcd9e08c6.json                                         Project_CP\n",
      "1     692a2b0bb818336d.json  Workflow constructed from history 'K22063917 A...\n",
      "2     3680984663c813e1.json  dhfr Bowtie2 output bowtie2 MPileup output_mpi...\n",
      "3     49f8b32c3206f76c.json  Workflow for Genomic Data Science with Galaxy ...\n",
      "4     25adc55d2a26e34b.json  GigaScience Example 1B -- aye-aye FST (importe...\n",
      "...                     ...                                                ...\n",
      "1009  d5cba8a5ba6880fd.json  BRACA2 - primer design exon Input dataset snp ...\n",
      "1010  e0da87cadb1e6d5f.json  2 peaks-result data file 2 and more peaks 16 c...\n",
      "1011  fbf75fbb72b488bd.json  'BBL735_Lab2(Olympic)_AT' olympics.tsv Input d...\n",
      "1012  0d8eb75d28fa7df2.json  Desanka Lazic_Project Coriell-NA12880_R2.fastq...\n",
      "1013  f8238234db6f04c3.json  handson Input dataset Input dataset Join outpu...\n",
      "\n",
      "[1014 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(list(dic_workflows.items()), columns=['name', 'text'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Carregar o modelo e o tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('google-bert/bert-large-uncased')\n",
    "model = BertModel.from_pretrained('google-bert/bert-large-uncased')\n",
    "\n",
    "texts = df['text'].tolist()\n",
    "\n",
    "# Função para converter texto em embedding\n",
    "def text_to_embedding(text):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    # Pegar a média dos embeddings de todos os tokens para representar o texto\n",
    "    return model_output.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Converter todos os textos para embeddings\n",
    "embeddings = [text_to_embedding(text) for text in texts]\n",
    "\n",
    "# Converter a lista de embeddings em um array 2D\n",
    "embeddings_array = np.vstack(embeddings)\n",
    "\n",
    "# Calcular a matriz de similaridade\n",
    "similarity_matrix = cosine_similarity(embeddings_array)\n",
    "\n",
    "# Identificar os índices dos três textos mais similares para cada texto\n",
    "similar_indices = similarity_matrix.argsort(axis=1)[:, :-5:-1]  # -4 porque os três mais similares excluem o próprio texto\n",
    "\n",
    "# Remover o índice do próprio texto\n",
    "corrected_similar_indices = []\n",
    "for idx, indices in enumerate(similar_indices):\n",
    "    filtered_indices = [index for index in indices if index != idx][:3]  # Exclui o próprio e pega os três mais similares\n",
    "    corrected_similar_indices.append(filtered_indices)\n",
    "\n",
    "# Criar coluna no DataFrame para os índices dos textos mais similares\n",
    "df['top3_description'] = corrected_similar_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyncoln/Git/similaridade_workflow/venv_linux/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Carregar o modelo e o tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "model = BertModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "\n",
    "texts = df['text'].tolist()\n",
    "\n",
    "# Função para converter texto em embedding\n",
    "def text_to_embedding(text):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt', max_length=512, padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    # Pegar a média dos embeddings de todos os tokens para representar o texto\n",
    "    return model_output.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Converter todos os textos para embeddings\n",
    "embeddings = [text_to_embedding(text) for text in texts]\n",
    "\n",
    "# Converter a lista de embeddings em um array 2D\n",
    "embeddings_array = np.vstack(embeddings)\n",
    "\n",
    "# Calcular a matriz de similaridade\n",
    "similarity_matrix = cosine_similarity(embeddings_array)\n",
    "\n",
    "# Identificar os índices dos três textos mais similares para cada texto\n",
    "similar_indices = similarity_matrix.argsort(axis=1)[:, :-5:-1]  # -4 porque os três mais similares excluem o próprio texto\n",
    "\n",
    "# Remover o índice do próprio texto\n",
    "corrected_similar_indices = []\n",
    "for idx, indices in enumerate(similar_indices):\n",
    "    filtered_indices = [index for index in indices if index != idx][:3]  # Exclui o próprio e pega os três mais similares\n",
    "    corrected_similar_indices.append(filtered_indices)\n",
    "\n",
    "# Criar coluna no DataFrame para os índices dos textos mais similares\n",
    "df['top3_description_scibert'] = corrected_similar_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "os.chdir(os.pardir)\n",
    "df.to_csv(\"top3_descricao.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_linux",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
